{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données lues à partir du fichier :  ï»¿Project Gutenberg's Among the Forest People, by Clara Dillingham Pierson\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "almost no restrictions whatsoever.  You may copy it, give \n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import nltk\n",
    "import spacy\n",
    "import sklearn\n",
    "\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "\n",
    "with open(os.getcwd() + \"/Data - NLP/Among the Forest People.txt\", 'r') as fh:\n",
    "    filedata = fh.read()\n",
    "\n",
    "print(\"Données lues à partir du fichier : \", filedata[0:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PlaintextCorpusReader in 'c:\\\\Users\\\\delph\\\\Documents\\\\Projects Epitech\\\\DATA\\\\C-DAT-500-LYN-2-3-nlp-timothee.gros'>\n"
     ]
    }
   ],
   "source": [
    "corpus = PlaintextCorpusReader(os.getcwd(), \"alice-in-wonderland.txt\")\n",
    "\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier dans le corpus : ['alice-in-wonderland.txt']\n",
      "\n",
      "Nombre total de paragraphs corpus : 2034\n",
      "\n",
      "Nombre total de phrases dans ce corpus :  2625\n",
      "\n",
      "Première phrase :  ['The', 'Project', 'Gutenberg', 'EBook', 'of', 'Alice', 'in', 'Wonderland', ',', 'by', 'Alice', 'Gerstenberg']\n",
      "\n",
      "Mots dans le corpus :  ['The', 'Project', 'Gutenberg', 'EBook', 'of', 'Alice', ...]\n"
     ]
    }
   ],
   "source": [
    "# Extraire id de fichier du corpus\n",
    "print(\"Fichier dans le corpus :\", corpus.fileids())\n",
    "\n",
    "# Extraire paragraphes du corpus\n",
    "paragraphs = corpus.paras()\n",
    "print(\"\\nNombre total de paragraphs corpus :\", len(paragraphs))\n",
    "\n",
    "# Extraire phrases du corpus\n",
    "sentences = corpus.sents()\n",
    "print(\"\\nNombre total de phrases dans ce corpus : \", len(sentences))\n",
    "print(\"\\nPremière phrase : \", sentences[0])\n",
    "\n",
    "# Extraire mots du corpus\n",
    "print(\"\\nMots dans le corpus : \", corpus.words()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 des mots du corpus :  [('.', 1221), (',', 935), ('the', 752), (\"'\", 682), ('I', 458), ('you', 438), ('and', 436), ('to', 395), ('a', 372), ('ALICE', 354)]\n",
      "\n",
      "Distribution pour le mots \"ALICE\" :  354\n"
     ]
    }
   ],
   "source": [
    "# Trouver la distribution de fréquence des mots dans le corpus\n",
    "course_freq_dist = nltk.FreqDist(corpus.words())\n",
    "\n",
    "# Mots les plus utilisés et leur nb de fois qu'ils apparaissent \n",
    "print(\"Top 10 des mots du corpus : \", course_freq_dist.most_common(10))\n",
    "\n",
    "# Trouver la distribution d'un mot spécifique\n",
    "print(\"\\nDistribution pour le mots \\\"ALICE\\\" : \", course_freq_dist.get(\"ALICE\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenisation de texte (processus décomposit° en mots, termes, symboles, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token list :  ['ï', '»', '¿Project', 'Gutenberg', \"'s\", 'Among', 'the', 'Forest', 'People', ',', 'by', 'Clara', 'Dillingham', 'Pierson', 'This', 'eBook', 'is', 'for', 'the', 'use']\n",
      "\n",
      "Total tokens :  40449\n"
     ]
    }
   ],
   "source": [
    "# Ouvrir file dans une variable de text brut\n",
    "base_file = open(os.getcwd() + \"/Data - NLP/Among the Forest People.txt\", 'r')\n",
    "filedata = base_file.read()\n",
    "base_file.close()\n",
    "\n",
    "# Extraire des tokens convert en une list de token\n",
    "token_list = nltk.word_tokenize(filedata)\n",
    "print(\"Token list : \", token_list[:20])\n",
    "print(\"\\nTotal tokens : \", len(token_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nettoyage du text (Formatage : dates, ponctuation, abréviation, conversion de CAS, hashtags/Mention>/url) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liste des tokens après supp ponctuation :  ['ï', '¿Project', 'Gutenberg', \"'s\", 'Among', 'the', 'Forest', 'People', 'by', 'Clara', 'Dillingham', 'Pierson', 'This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone']\n",
      "\n",
      "Total des tokens aprè supp ponctuation :  34663\n"
     ]
    }
   ],
   "source": [
    "# Utilisation package punkt pour extraire les tokens\n",
    "# nltk.download('punkt')\n",
    "  \n",
    "# is no punct permet de supp ponctuation, avec un lambda token \n",
    "\n",
    "token_list2 = list(filter(lambda token: nltk.tokenize.punkt.PunktToken(token).is_non_punct, token_list)) \n",
    "print(\"Liste des tokens après supp ponctuation : \", token_list2[:20])\n",
    "print(\"\\nTotal des tokens aprè supp ponctuation : \", len(token_list2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertir token en minusculeles tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List tokens après conversion en lower :  ['ï', '¿project', 'gutenberg', \"'s\", 'among', 'the', 'forest', 'people', 'by', 'clara', 'dillingham', 'pierson', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone']\n",
      "\n",
      "Total tokens après conversion en lower :  34663\n"
     ]
    }
   ],
   "source": [
    "token_list3 =[word.lower() for word in token_list2]\n",
    "print(\"List tokens après conversion en lower : \", token_list3[:20])\n",
    "print(\"\\nTotal tokens après conversion en lower : \", len(token_list3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppression des mots vides (grpe de mot pas sens par eux-mêmes like : le, et, dans, etc.) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List tokens après supp mots vides :  ['ï', '¿project', 'gutenberg', \"'s\", 'among', 'forest', 'people', 'clara', 'dillingham', 'pierson', 'ebook', 'use', 'anyone', 'anywhere', 'cost', 'almost', 'restrictions', 'whatsoever', 'may', 'copy']\n",
      "\n",
      "Total tokens après supp mots vides :  16023\n"
     ]
    }
   ],
   "source": [
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Remove stopwords\n",
    "token_list4 = list(filter(lambda token : token not in stopwords.words('english'), token_list3))\n",
    "print(\"List tokens après supp mots vides : \", token_list4[:20])\n",
    "print(\"\\nTotal tokens après supp mots vides : \", len(token_list4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Racinisation (Stemming) (barre de base du mot, ex : \"combiner\" est le radical de : combine, combinant, combiné):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List tokens après le stemming :  ['ï', '¿project', 'gutenberg', \"'s\", 'among', 'forest', 'peopl', 'clara', 'dillingham', 'pierson', 'ebook', 'use', 'anyon', 'anywher', 'cost', 'almost', 'restrict', 'whatsoev', 'may', 'copi']\n",
      "\n",
      "Total tokens après stemming :  16023\n"
     ]
    }
   ],
   "source": [
    "# Utiliser le package SnowballStemmer pour la radicalisation\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# Stem data\n",
    "token_list5 = [stemmer.stem(word) for word in token_list4]\n",
    "print(\"List tokens après le stemming : \", token_list5[:20])\n",
    "print(\"\\nTotal tokens après stemming : \", len(token_list5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatisation (similaire au stemming MAIS produit un mot racine ex : \"combinant\" devient \"combiner\")\n",
    "Utilise un dictionnaire pour faire correspondre les mots à leur mot racine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\delph\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\delph\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List tokens après lemmatisation :  ['ï', '¿project', 'gutenberg', \"'s\", 'among', 'forest', 'people', 'clara', 'dillingham', 'pierson', 'ebook', 'use', 'anyone', 'anywhere', 'cost', 'almost', 'restriction', 'whatsoever', 'may', 'copy']\n",
      "\n",
      "Total tokens après lemmatisation :  16023\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "token_list6 = [lemmatizer.lemmatize(word) for word in token_list4]\n",
    "print(\"List tokens après lemmatisation : \", token_list6[:20])\n",
    "print(\"\\nTotal tokens après lemmatisation : \", len(token_list6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparaison tokens entre stemming et lemmatisation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw :  anyone  , Stemmed :  anyon  , Lemmatized :  anyone\n"
     ]
    }
   ],
   "source": [
    "# Verif tokens :\n",
    "print(\"Raw : \", token_list4[12],\" , Stemmed : \", token_list5[12], \" , Lemmatized : \", token_list6[12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-grammes : séquence de n éléments dans un échantillon de text\n",
    "N-grammes : bigrammes, trigrammes etc.\n",
    "Exemple : \"La vie est belle\" :\n",
    "- Bigrammes : (la, vie), (vie, est), (est, belle) \n",
    "- Trigrammes : (la, vie, est), (vie, est, belle)\n",
    "\n",
    "Utiliser pour prédire la prochaine séquence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrammes les plus courants : \n",
      "[(('ground', 'hog'), 74), (('project', 'gutenberg-tm'), 57), (('blue', 'jay'), 48), (('red', 'squirrel'), 40), (('project', 'gutenberg'), 29)]\n",
      "\n",
      "Trigrammes les plus courants : \n",
      "[(('mr.', 'red', 'squirrel'), 27), (('great', 'horn', 'owl'), 18), (('project', 'gutenberg-tm', 'electron'), 18), (('gutenberg-tm', 'electron', 'work'), 18), (('biggest', 'littl', 'rabbit'), 15)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "# Trouver des bigrammes\n",
    "bigrams = ngrams(token_list5, 2)\n",
    "print(\"Bigrammes les plus courants : \")\n",
    "print(Counter(bigrams).most_common(5))\n",
    "# Trouver des trigrammes\n",
    "trigrams = ngrams(token_list5, 3)\n",
    "print(\"\\nTrigrammes les plus courants : \")\n",
    "print(Counter(trigrams).most_common(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of speech Tagging (POS Tagging) : identifie la partie du discours pour chaque mot dans un corpus\n",
    "\n",
    "|   Mots    |    POS    | Descript° |\n",
    "|-----------|-----------|-----------|\n",
    "|   Petit   |     JJ    | Adjective |\n",
    "|   Chat    |     NN    |    Nom    |\n",
    "|   Miaule  |    VBP    |Verbe sing |\n",
    "\n",
    "Utiliser pour la reconnaissance des entités, filtrage et analyse des sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A REVOIR / SPACY A LA MAISON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ï', 'NN'),\n",
       " ('¿project', 'NNP'),\n",
       " ('gutenberg', 'NN'),\n",
       " (\"'s\", 'POS'),\n",
       " ('among', 'IN'),\n",
       " ('forest', 'JJ'),\n",
       " ('peopl', 'NN'),\n",
       " ('clara', 'NN'),\n",
       " ('dillingham', 'NN'),\n",
       " ('pierson', 'NN')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dl package tagger\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# from spacy.language import Language\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Tag les 10 premiers tokens\n",
    "nltk.pos_tag(token_list5)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag text en utilisant spacy\n",
    "doc = nlp(\" \".join(token_list5))\n",
    "pos_tags = [(d.text, d.pos_) for d in doc]\n",
    "\n",
    "# Premiers mots\n",
    "print(pos_tags[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term Frequency-Inverse Document Frequency (TF-IDF) :\n",
    "- Représentation numérique du texte\n",
    "- Utiliser pour convertir du texte en une représentation de tableau numérique\n",
    "- Sorti d'un tableau avec lignes -> documents et colonnes -> mots\n",
    "- Chaque cellule fournit une valeur qui rpz la force relative du mot par rapport au document\n",
    "\n",
    "Exemple :\n",
    "Corpus de 3 documents :\n",
    " - Doc1 = \"Ceci est un exemple de bons mots\"\n",
    " - Doc2 = \"Il a répété encore et encore le même mot après mot\"\n",
    " - Doc3 = \"Les mots peuvent vraiment blesser\"\n",
    "\n",
    "Après nettoyage :\n",
    " - Doc1 = \"exemple bon mot\"\n",
    " - Doc2 = \"répéter encore encore même mot mot\"\n",
    " - Doc3 = \"mot vraiment blesser\"\n",
    "\n",
    " Création d'une table de comptage :\n",
    "\n",
    "|          |  exemple  | bon       |   mot     |  répéter  | même      | vraiment | blesser | encore |\n",
    "|----------|-----------|-----------|-----------|-----------|-----------|----------|---------|--------|\n",
    "|   Doc1   |     1     |   1       |     1     |           |           |          |         |        |\n",
    "|   Doc2   |           |           |     2     |     1     |     1     |          |         |   2    |\n",
    "|   Doc3   |           |           |     1     |           |           |    1     |    1    |        |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Trouver la fréquence du text (TF) :\n",
    "\n",
    "|          |  exemple  | bon       |   mot     |  répéter  | même      | vraiment | blesser | encore |\n",
    "|----------|-----------|-----------|-----------|-----------|-----------|----------|---------|--------|\n",
    "|   Doc1   |  0.33     |   0.33    |    0.33   |           |           |          |         |        |\n",
    "|   Doc2   |           |           |    0.4    |     0.2   |    0.2    |          |         |  0.4   |\n",
    "|   Doc3   |           |           |   0.33    |           |           |   0.33   |    0.33 |        |\n",
    "\n",
    " Trouver la fréquence inverse documents (IDF) :\n",
    " - Log e (total docs/docs avec le mot)\n",
    "\n",
    "|          |  exemple  | bon       |   mot     |  répéter  | même      | vraiment | blesser | encore |\n",
    "|----------|-----------|-----------|-----------|-----------|-----------|----------|---------|--------|\n",
    "|   IDF    |  1.098    |   1.098   |   0       | 1.098     |   1.098   |    1.098 | 1.098   | 1.098  |\n",
    "\n",
    " Trouver TF-IDF : \n",
    " - TF x IDF pour ces mots\n",
    "\n",
    " Exemple : 0.33 x 1.098\n",
    " \n",
    "|          |  exemple  | bon       |   mot     |  répéter  | même      | vraiment | blesser | encore |\n",
    "|----------|-----------|-----------|-----------|-----------|-----------|----------|---------|--------|\n",
    "|   Doc1   |  0.36     |   0.36    |    0      |           |           |          |         |        |\n",
    "|   Doc2   |           |           |    0      |    0.21   |   0.21    |          |         |  0.43  |\n",
    "|   Doc3   |           |           |   0       |           |           |   0.36   |  0.36   |        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les tokens utilisés comme fonctionnalités sont : \n",
      "['analyse' 'collecte' 'conseils' 'données' 'démarrer' 'fouille'\n",
      " 'nettoyage' 'python' 'sentiments' 'simplifiée' 'texte']\n",
      "\n",
      "Taille du tableau, chaque ligne rpz document, chaque colonne rpz token\n",
      "(3, 11)\n",
      "\n",
      "Tableau TF-IDF réel\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.49047908, 0.        , 0.49047908,\n",
       "        0.37302199, 0.        , 0.49047908, 0.        , 0.        ,\n",
       "        0.37302199],\n",
       "       [0.        , 0.49047908, 0.        , 0.49047908, 0.        ,\n",
       "        0.37302199, 0.49047908, 0.        , 0.        , 0.        ,\n",
       "        0.37302199],\n",
       "       [0.57735027, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.57735027, 0.57735027,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Utiliser scikit learn \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Utiliser un petit corpus pour chaque visualisation\n",
    "vector_corpus = [\n",
    "    'Fouille du texte en R et en Python : 8 conseils pour démarrer',\n",
    "    'Fouille du texte en R : collecte et nettoyage de données',\n",
    "    'L\\'analyse des sentiments dans R simplifiée',\n",
    "]\n",
    "\n",
    "# Créer un vectoriseur pour la langue / stopwords mots vides en english\n",
    "vectorizer = TfidfVectorizer(stop_words = stopwords.words('french'))\n",
    "\n",
    "# Créer le vecteur / TD IDF\n",
    "tfidf = vectorizer.fit_transform(vector_corpus)\n",
    "\n",
    "# Affiche les mots/noms en vedette\n",
    "print(\"Les tokens utilisés comme fonctionnalités sont : \")\n",
    "print(vectorizer.get_feature_names_out())\n",
    "\n",
    "# Affiche dimension tableau\n",
    "print(\"\\nTaille du tableau, chaque ligne rpz document, chaque colonne rpz token\")\n",
    "print(tfidf.shape)\n",
    "\n",
    "# Affiche tableau\n",
    "print(\"\\nTableau TF-IDF réel\")\n",
    "tfidf.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Stockage de données textuelles\n",
    " - Utiliser un stockage Big Data adapté au format libre pour le textuelles :\n",
    "    - Stockage en nuage HDF3, S3 ou Google\n",
    " - Créer des index sur éléments de données clés pour un accès facile\n",
    "    - MongoDB\n",
    "    - ElasticSearch\n",
    " - Stocker le text traité comme les tokens et TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dcd1a956b512f6ee4e1c1990de27de777286bc7b66f403046b31b83967e8a9e6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
